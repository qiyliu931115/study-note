传统定义
    
    发布订阅 消息队列 

最新定义

    流平台 做数据存储和计算

使用场景

    缓存削峰，解耦，异步通信

两种模式

    点对点 一个producer 一个consumer 一个topic 会删除数据

    发布订阅  多个producer 多个consumer 多个topic 不会删除数据

架构

    producer

        对接外部海量的数据

    broker 

        1.服务器
        2.topic 对数据分类
        3.partition 分区
        4.保证数据的可靠性 使用副本
        5.副本分为leader和follower，producer和consumer只对leader进行读写数据

    consumer

        1.consumer和consumer相互独立
        2.消费组 一个组只会消费某一个topic 一个topic的partition只能由一个consumer消费        

    zookeeper 

        1.不使用Kafka内部的zookeeper，使用外部公用的 结合Hadoop Hbase的zookeeper进行使用。
        2.存储brokerIds 
        3.每个topic下的每一个partition的leader


安装

    brokerId必须全局唯一
    配置文件修改brokerId, log.dirs存储位置默认temp目录，必须修改，连接对应的zookeeper
    启动，停止，要先停止Kafka，再停止zookeeper

    常用命令行
        kafka-topic.sh

        kafka-console-producer.sh

        kafka-console-consumer.sh

生产者producer

    kafkaProducer

    send方法

    拦截器

    序列化

    分区器

    缓存队列 默认32MB 每批次数据是16k
        
    sender线程发送数据 batch size和linger ms控制何时发送

    sender向broker发送请求会缓存五个请求，如果broker都没应答，就不会再发送请求

    sender里面使用的是selector处理IO流
    
    broker应答使用ack， 0-不用等 1-leader落盘应答 -1是leader和isr都落盘

    发送请求失败重试是integer.MAX

    发送方式

        send异步 send.get同步 send(.new callback())异步回调


分区

    分区可以把海量数据切割各服务器

    分区规则

        指定分区 

        指定key 按key的hashcode % 分区数

        没指定分区 也没指定key 粘性分区

        粘性分区 随机指定key 切换时随机

    自定义分区
    
        指定类 实现partitioner接口

        根据消息内容实现指定发送到哪个分区

    吞吐量如何提高

        batch size

        linger ms
        
        对消息压缩

        缓冲队列大小提高

    可靠性

        acks 
        0   丢失
        1   leader挂了 也可能丢失数据
        -1（all） 副本大于等于二，isr >=2 

    数据重复

        幂等性 默认开启，<pid（每次Kafka重启都会分配新的pid）,分区号，序列号>
            
        事务
            基于幂等性

            事务ID，手动设置 全局唯一

            事务协调器处理

    数据有序

        单会话单分区有序

        多分区有序 在consumer处理

    数据乱序
    
        1.x前inflight设置为1
        
        未幂等性 inflight设置为1
        
        开启幂等性 broker根据消息的序列号（单调递增的）对最多对五条数据进行缓冲并且重排序

        排序后在发送给consumer

broker

    zookeeper存储了哪些信息

        brokerIds

        partition分区的leader信息

        负责选举  controller

    工作流程

        broker上线 ，brokerids增加数据

        多个broker竞争controller辅助选择 监控zookeeper的ids （观察者模式）

        从isr中或者，ar(all replica)最前面的 选出leader

        leader接收数据后，follower拉取同步数据

        持久化数据到硬盘中的log中的segment，每个segment的大小是1G（底层搜索使用了索引）

        leader挂掉 controller监听到后，重新选举
        
        从isr中或者，ar(all replica)最前面的 再次选出leader

副本 
    
    提高可靠性

    生产环境设置两个 默认一个

    有leader和follower之分

    所有生产者和消费者针对的对象是leader

    所有与leader副本保持一定程度同步的副本（包括leader副本在内）组成ISR（In-Sync Replicas），ISR集合是AR集合中的一个子集。

    分区中的所有副本统称为AR（Assigned Replicas）

    与leader副本同步滞后过多的副本（不包括leader副本）组成OSR（Out-of-Sync Replicas）
        
    从controller进行选举leader （在isr中存活，优先选取在ar排名靠前的）

    leader挂了 从controller重新进行选举leader 

    重新选举的leader要解决数据不一致 同步问题

    LEO 每个副本最后一个offset +1  HW水位线 最低的副本的offset +1

    follower挂掉，如果想加入到isr，需要再次同步leader的数据 将 HW 高于log中的截取掉 重新拉取leader数据 直到追上分区的HW 

    就可以重新加入isr

存储机制

    broker 
    topic->partition
    log->segment 每个segment 记录着上一个segment的最后一个数据地址偏移量加1

    稀疏索引 每存储4KB的数据，记录一条索引

删除数据

    保存数据的默认日期是七天 一般生产环境配置3天 像flink数仓那种有的只有7小时

    到期后直接删除 或者 压缩

    删除：如果segment中数据有部分过期 部分没过期 那需要等数据都过期了才会删除

    压缩：compact 按每个key最新的value进行保存

高效读写

    集群模式 使用分区技术 提高生产端和消费端的并行度

    稀疏索引作为存储机制 按顺序读写数据

    使用零拷贝 和 页缓存

    避免线程在内核态和用户态切换的开销，使用内核空间的缓存，避免用户空间和内核空间的数据拷贝

    将内核中缓存直接发送到网卡（缓存没有使用顺序寻址，读取硬盘数据（使用稀疏索引））

消费者
    
    总体流程

    消费者组

    coordinator收集到组内所有consumer的请求 选出leader 

    coordinator发送消息给leader leader制定分配规则（range，round robin, sticky粘性）

再平衡

    consumer长时间未和coordinator通信，或者处理消息时间过长，会被踢出消费组 触发再平衡。
    
    有新的consumer加入消费组也会触发再平衡

    同样的 如果分区数发生变化（只能变大不能变小）也会触发再平衡

consumer抓取数据

    max.poll.records设置每次拉取数量 可设置每次拉取的最小（1kb）和最大数据大小50MB

    间隔固定时间也会去拉取数据   

    经过反序列化和拦截器 consumer开始处理数据


offset

    默认存储再系统主题 _consumer_offset

    自动提交 5s

    手动提交 同步，异步

    手动指定offset 指定offset位置消费数据 或者指定时间从某个时间的开始消费

事务

    生产端 幂等性开启 ACK=-1 副本大于等于2 ISR大于等于2

    broker到消费者 开启手动提交offset

    如果数据持久化如到数据库 开启数据库事务