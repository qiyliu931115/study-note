
见2.4. Distillation: Empower Small Models with Reasoning Capability

直接用DeepSeek-R1阶段三：“拒绝采样和SFT” 时的数据对小模型进行SFT，不包含RL阶段，就能取得比较好的效果。

### 小模型+RL不如直接蒸馏？

    从DeepSeek的实验结果来看，直接蒸馏通常优于小模型直接进行强化学习。

### 蒸馏的优势

1. 性能提升显著：蒸馏能够将大模型的推理能力有效地迁移到小模型中，显著提升小模型的推理性能。例如，将DeepSeek-R1的推理能力蒸馏到Qwen-32B模型上，其表现显著优于直接在Qwen-32B上进行大规模RL训练。
2. 计算成本低：蒸馏过程相对简单，主要依赖于模仿学习（Mimicking Loss），不需要像RL那样进行大规模的探索和训练。这使得蒸馏在计算资源和时间成本上更具优势。
3. 稳定性更高：RL训练的小模型容易陷入局部最优，而蒸馏则直接继承了大模型的成熟策略，准确率和稳定性更高。

### 强化学习的局限性

1. 探索效率低：小模型在RL训练过程中，由于探索空间较大，容易陷入局部最优，难以学习到复杂的推理策略。
2. 计算成本高：RL训练需要大量的计算资源和时间，通常需要在大规模集群上进行训练，而蒸馏则可以在单机上完成。
3. 性能提升有限：在相同的计算预算和训练轮次下，RL训练的小模型性能提升不如蒸馏显著。