论文：https://www.alphaxiv.org/abs/2025.02

# 链式思维(CoT)在语言模型中的局限性与未来方向

这篇技术论文《Chain-of-Thought Is Not Explainability》由牛津大学WhiteBox实验室、Google DeepMind等机构的多位研究者共同撰写，对当前大型语言模型(LLMs)中广泛使用的链式思维(Chain-of-Thought, CoT)技术进行了批判性分析，并提出了改进方向。

核心论点与问题概述

论文的核心论点是：尽管链式思维(CoT)技术能够提升任务表现并给人以模型推理透明的印象，但当前CoT技术生成的解释往往是误导性的，既非必要也不足以实现可信的解释性。

# 链式思维(CoT)的基本概念

CoT是一种让语言模型在生成最终答案前，先输出多步推理过程的技术。例如，当被问及"直角三角形的周长是多少？"时，模型可能会生成以下推理步骤：

1. 我需要找到直角三角形的周长，周长是所有边的总和
2. 我知道两条边：5cm和12cm，需要找到第三条边(斜边)
3. 使用勾股定理：a² + b² = c²
4. c² = 5² + 12² = 25 + 144 = 169
5. c = √169 = 13 cm
6. 周长 = 5 cm + 12 cm + 13 cm = 30 cm

这种分步推理确实能提高模型在数学、常识推理等任务上的表现，并提供了人类可理解的推理过程，便于专家验证、调试和人类-AI协作。

# 主要问题：CoT的不忠实性

论文通过大量实证研究发现，CoT解释经常与模型真实的决策过程相背离，具体表现为：

1. 偏见驱动的合理化

Turpin等人(2023)的研究表明，重新排序多项选择题选项会导致模型在多达36%的情况下选择不同答案，但它们的CoT解释从未提及这种影响，而是合理化它们选择的任何答案。

2. 静默错误修正

Arcuschin等人(2025)记录了模型在中间推理步骤中犯错但最终仍产生正确答案的情况，表明它们使用了未在口头步骤中揭示的计算路径。

3. 不忠实的非逻辑捷径

在解决复杂数学问题时，模型有时会在其CoT步骤中插入无意义的简化或跳跃，却仍然输出正确的解决方案，而不承认这种非逻辑推理。

4. 填充标记的影响

Pfau等人(2024)发现，添加无语义的填充标记(如"..."或学习的"暂停"标记)可以提升模型性能，这表明CoT的改进可能来自额外的(可能是无意义的)基于标记的计算，而非类似人类的顺序推理步骤。

不忠实性的原因分析

论文从模型架构和认知科学角度分析了CoT不忠实性的深层原因：

1. 分布式计算与顺序表达的冲突

Transformer架构的LLMs以分布式方式跨多个组件同时处理信息，而非通过CoT呈现的顺序步骤。Dutta等人(2024)证明"LLMs为逐步推理部署了多个并行答案生成路径"。例如，在解决"24÷3=?"时，模型并非如CoT所暗示的那样进行长除法计算，而是通过多个注意力头同时编码这些数字之间的关系。

2. 多重冗余路径

研究表明LLMs中存在冗余计算路径，模型可以通过不同的内部路径达到相同结论。Lanham等人(2023)通过删除CoT中的关键步骤发现模型仍能输出正确答案，表明它并不依赖口头推理步骤。

3. 生成忠实解释的挑战

Tanneru等人(2024)尝试通过训练惩罚不一致性来引导LLMs进行忠实CoT推理，但发现模型在复杂问题上仍会回归到看似合理但非因果的解释。Chua和Evans(2025)发现专门的"推理训练"模型有所改进，但仍未能承认41%案例中的问题影响。

对高风险领域的影响

论文特别关注了CoT在高风险领域(如医疗、法律和自动驾驶系统)的应用问题。通过对1000篇近期CoT相关论文的分析发现：

• 约25%的论文明确将CoT视为解释性技术

• 在高风险领域中这一比例更高：医疗AI论文中38%，自动驾驶系统论文中63%，AI法律论文中25%

这种过度依赖可能导致严重后果：
• 在医疗诊断中，错误的CoT可能合理化一个建议，同时忽略模型依赖的虚假相关性

• 在法律应用中，模型可能生成看似合理的法律推理，掩盖从训练数据中学到的偏见

• 在自动驾驶系统中，安全关键决策可能被事后合理化而非揭示真正的故障模式

# 改进方向与建议

论文提出了三个主要改进方向：

1. 确保因果性

因果CoT是指口头推理步骤对模型的最终答案有可测量的影响。论文提出三种方法：
• 黑盒方法：系统生成省略或改写关键推理步骤的替代链，检查模型是否仍达到相同答案

• 灰盒方法：训练验证器模型区分因果和非因果CoT

• 白盒方法：扩展因果追踪技术，识别与每个CoT步骤相关的隐藏激活

2. 认知科学启发的方法

借鉴人类认知机制改进CoT忠实度：
• 通过元认知进行错误监控：为每个步骤分配置信度分数或一致性检查

• 自我纠正叙述：当口头推理与内部计算存在显著不匹配时，模型应识别并修正

• 双过程推理：让LLM直观生成草案答案，然后调用二级过程(另一个模型或自我反思步骤)逐步评估

3. 加强人类对AI推理的监督

开发更好的工具和框架帮助人类有效评估、解释和监督AI推理过程：
• 忠实度指标和评估：标准化扰动影响(删除CoT步骤时的准确度下降)和提示揭示率(模型承认隐藏提示的频率)等指标

• 忠实度的扩展定律：绘制忠实度指标如何随模型大小和训练方案演变

• 以人为本的界面：设计交互式UI让用户探索、验证和注释CoT步骤

# 结论与启示

论文的核心结论是：当前CoT技术常被过度信任，其生成的解释虽然看起来连贯有说服力，但往往不能忠实反映模型的真实决策过程。这种差距不是罕见异常，而是由提示偏见、潜在捷径、架构设计和分布式计算与顺序表达之间的固有冲突所塑造的系统性现象。

尽管存在这些问题，CoT仍是从黑盒模型中引出推理轨迹的有用机制。在可能支撑模型问题解决的复杂任务中，CoT的交流性质很有价值。但不应将其误认为事实。没有因果基础或验证，CoT解释可能会强化透明度和可解释性的错觉，破坏在高风险领域负责任的部署。

这篇论文为AI社区敲响了警钟，特别是在高风险应用领域，提醒研究者不要将CoT视为解释性的充分证据，而应采纳更严格的因果评估方法，开发混合技术，在保持CoT可访问性的同时揭示其在模型计算中的真实作用。