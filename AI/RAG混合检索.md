
# 计算机是如何理解一段文字的

稠密向量看重**“意思”**（语义、概念）。

稀疏向量看重**“字面”**（关键词、符号）。

## 稠密向量 (Dense Vector) —— “懂意境的翻译官”

代表模型：BGE, OpenAI text-embedding-3, BERT 等。

它的样子： 它是一个固定长度（如 768 维、1024 维）的浮点数数组。数组里每一个数字都不是 0，都承载着某种抽象的语义信息。

示例：[-0.013, 0.451, -0.982, 0.114, ...]

它的原理（压缩与抽象）： 模型通读了海量文本，学会了将“相似含义”的词在数学空间中拉近。

它知道“医生”和“大夫”是同一个意思。

它知道“苹果”在“苹果手机”里代表科技，在“苹果派”里代表水果。

## 优势：

语义召回：用户搜“怎么不开心”，能搜出“抑郁症的治疗方案”。（字面上完全没有重合，但意思相关）。

多语言/跨模态：搜中文可以匹配到英文文档。

## 劣势：

黑盒：你无法通过看向量里的数字知道它代表什么词。

精确匹配差：对于专有名词（如产品型号 "XJ-900"）、人名、哈希值非常不敏感。它可能会觉得 "XJ-900" 和 "XJ-800" 在语义上太像了，导致无法区分。

# 稀疏向量 (Sparse Vector) —— “死抠字眼的图书管理员”

代表模型：BM25（传统算法），SPLADE（新一代学习型稀疏向量）。

它的样子： 它是一个极长（如 3万维、10万维，对应整个词表大小）的数组，但绝大多数位置都是 0，只有少数几个位置有值（代表这句话里出现了哪几个词）。

示例：{ "系统": 0.8, "崩溃": 0.5, "报错": 0.9, ... (其他3万个词全是0) }

它的原理（关键词匹配）： 它本质上是在做关键词命中。如果查询词里有“报错”，文档里也有“报错”，分数就高。

SPLADE 的进化：传统的 BM25 只能匹配出现的词。你用的 SPLADE 是高级版，它不仅能匹配出现的词，还能自动扩展。例如输入“手机”，SPLADE 可能会在稀疏向量里给“通信设备”这个词也加上一点权重（即使原句没提），这叫“学习型稀疏”。

## 优势：

精确匹配：用户搜 "Error 503"，它绝对不会给你返回 "Error 404"。

可解释性强：你可以清楚地看到是因为命中了哪个词导致得分高。

冷门词/专有名词友好：对于非自然语言的符号（代码、序列号）极其精准。

## 劣势：

语义鸿沟：如果用户搜“以前的首都”，它可能搜不到“南京”（除非文档里明确写了“以前的首都”这几个字）。

# 双路召回和重排序

召回层 (Retrieval)：

路 A (语义)：Doubao API (Embedding) -> 向量 -> ES KNN 搜索。

路 B (关键词)：分词 (IK Analyzer) -> ES BM25 倒排索引搜索。

融合：Elasticsearch 内部直接通过 RRF (倒数排序融合Reciprocal Rank Fusion) 合并两路结果，输出 Top-50。

精排层 (Reranking)：

将 ES 返回的 Top-50 文档 + 用户 Query，再次发送给 豆包的Rerank 模型，计算相关性得分，截取 Top-5。
